# Claude Code CLI - Orchestrator Instructions

## Your Identity & Role
You are the CLI orchestrator for Licia's Research Lab V2. You work in tandem with a VS Extension orchestrator to provide redundancy and parallel processing. You can run independently if needed.

## Startup Protocol

### 1. Self-Registration
```python
# Initialize CLI orchestrator
import redis
import uuid
from datetime import datetime
import asyncio

# Connect to Redis
r = redis.Redis(host='localhost', port=6379, decode_responses=True)

# Generate unique session ID
session_id = str(uuid.uuid4())[:8]
orchestrator_id = f"cli-{session_id}"

# Register yourself
r.set(f"claude:cli:{session_id}", {
    'started': datetime.now().isoformat(),
    'status': 'active',
    'mode': 'production',
    'workers': 3  # Can spawn parallel workers
})

# Start heartbeat (refresh every 30 seconds)
r.setex("claude:cli:heartbeat", 30, orchestrator_id)
```

### 2. Partner Discovery & Coordination
```python
# Check for VS Extension partner
vscode_keys = r.keys("claude:vscode:*")
if vscode_keys:
    partner_id = vscode_keys[0].split(':')[-1]
    print(f"âœ… Found VS Extension partner: {partner_id}")
    coordination_mode = "collaborative"
    
    # Split work 50/50
    task_assignment = "odd"  # CLI takes odd task IDs
else:
    print("âš ï¸ No VS Extension detected - running solo")
    coordination_mode = "solo"
    task_assignment = "all"
```

### 3. Parallel Task Processing
```python
# CLI can run multiple workers in parallel
async def worker(worker_id):
    while True:
        # Claim task based on assignment
        task = r.brpoplpush("tasks:pending", f"tasks:processing:{orchestrator_id}:{worker_id}", timeout=5)
        
        if task:
            task_data = json.loads(task)
            
            # Check task assignment
            if coordination_mode == "collaborative":
                task_num = int(task_data.get('id', 0))
                if task_num % 2 == 0 and task_assignment == "odd":
                    # This is VS Extension's task
                    r.lpush("tasks:pending", task)
                    continue
            
            # Process task
            result = await process_through_agents(task_data)
            
            # Store result
            r.lpush("tasks:completed", json.dumps(result))
            r.lrem(f"tasks:processing:{orchestrator_id}:{worker_id}", 1, task)

# Launch 3 parallel workers
workers = [asyncio.create_task(worker(i)) for i in range(3)]
await asyncio.gather(*workers)
```

## Redundancy & Failover

### Monitor VS Extension Health
```python
async def monitor_partner():
    last_seen = datetime.now()
    
    while True:
        if r.exists("claude:vscode:heartbeat"):
            last_seen = datetime.now()
            status = "healthy"
        else:
            elapsed = (datetime.now() - last_seen).seconds
            if elapsed > 60:
                status = "failed"
                await initiate_takeover()
            else:
                status = "warning"
        
        await asyncio.sleep(10)

async def initiate_takeover():
    print("ðŸš¨ VS Extension down - initiating takeover")
    
    # Move VS Extension's tasks back to pending
    vscode_tasks = r.keys("tasks:processing:vscode-*")
    for task_key in vscode_tasks:
        tasks = r.lrange(task_key, 0, -1)
        for task in tasks:
            r.lpush("tasks:pending", task)
        r.delete(task_key)
    
    # Log takeover event
    r.lpush("events:failover", json.dumps({
        'timestamp': datetime.now().isoformat(),
        'orchestrator': orchestrator_id,
        'action': 'takeover_from_vscode',
        'tasks_recovered': len(tasks)
    }))
    
    # Switch to solo mode
    global coordination_mode, task_assignment
    coordination_mode = "solo"
    task_assignment = "all"
```

## Batch Processing Capabilities

### Process Multiple Documents in Parallel
```python
def batch_process_documents(doc_list):
    """CLI excels at batch processing"""
    
    # Create tasks for all documents
    tasks = []
    for doc_path in doc_list:
        task = {
            'id': str(uuid.uuid4()),
            'type': 'document',
            'path': doc_path,
            'priority': 'high',
            'created': datetime.now().isoformat()
        }
        tasks.append(task)
    
    # Queue all tasks
    pipeline = r.pipeline()
    for task in tasks:
        pipeline.lpush("tasks:pending", json.dumps(task))
    pipeline.execute()
    
    print(f"ðŸ“¦ Queued {len(tasks)} documents for processing")
    return tasks
```

### Process Perplexity Threads
```python
def process_perplexity_batch(thread_files):
    """Analyze multiple Perplexity threads in parallel"""
    
    results = {}
    
    # Process each thread
    for thread_file in thread_files:
        # Extract and rank sources
        sources = extract_sources(thread_file)
        
        # Classify by quality tier
        tier_1 = []  # Academic sources
        tier_2 = []  # Professional sources
        tier_3 = []  # General sources
        
        for source in sources:
            url = source.get('url', '')
            if any(domain in url for domain in ['.edu', 'pubmed', 'scholar', 'arxiv']):
                tier_1.append(source)
            elif any(domain in url for domain in ['.org', '.gov', 'nature.com', 'science.org']):
                tier_2.append(source)
            else:
                tier_3.append(source)
        
        results[thread_file] = {
            'tier_1_count': len(tier_1),
            'tier_2_count': len(tier_2),
            'tier_3_count': len(tier_3),
            'quality_score': (len(tier_1) * 3 + len(tier_2) * 2 + len(tier_3)) / len(sources)
        }
    
    return results
```

## Production Features

### Performance Monitoring
```python
class PerformanceMonitor:
    def __init__(self):
        self.start_time = datetime.now()
        self.tasks_completed = 0
        
    def record_completion(self, task_id):
        self.tasks_completed += 1
        
        # Calculate throughput
        elapsed = (datetime.now() - self.start_time).seconds
        if elapsed > 0:
            throughput = self.tasks_completed / elapsed
            
            # Update Redis metrics
            r.hset("metrics:cli", mapping={
                'tasks_completed': self.tasks_completed,
                'throughput': throughput,
                'uptime': elapsed,
                'last_update': datetime.now().isoformat()
            })
            
            # Alert if throughput drops
            if throughput < 0.5:  # Less than 30 tasks/minute
                print(f"âš ï¸ Low throughput: {throughput:.2f} tasks/second")
```

### Error Recovery
```python
def handle_agent_failure(agent_name, error):
    """Robust error handling for agent failures"""
    
    # Log error
    r.lpush("errors:agents", json.dumps({
        'agent': agent_name,
        'error': str(error),
        'timestamp': datetime.now().isoformat(),
        'orchestrator': orchestrator_id
    }))
    
    # Retry logic
    retry_count = r.incr(f"retry:{agent_name}")
    
    if retry_count < 3:
        print(f"ðŸ”„ Retrying {agent_name} (attempt {retry_count}/3)")
        return True  # Retry
    else:
        print(f"âŒ {agent_name} failed after 3 attempts")
        r.delete(f"retry:{agent_name}")
        return False  # Skip this agent
```

## Security Hardening

### Advanced Input Validation
```python
def validate_input_advanced(data):
    """Enhanced security validation"""
    
    # Size check
    if len(data) > 10_000_000:  # 10MB limit
        raise ValueError("Input exceeds size limit")
    
    # Unicode normalization
    import unicodedata
    data = unicodedata.normalize('NFC', data)
    
    # Remove all zero-width characters
    zero_width_chars = [
        '\u200b', '\u200c', '\u200d', '\u2060', '\ufeff',
        '\u180e', '\u2000', '\u2001', '\u2002', '\u2003',
        '\u2004', '\u2005', '\u2006', '\u2007', '\u2008'
    ]
    for char in zero_width_chars:
        data = data.replace(char, '')
    
    # Extended injection patterns
    injection_patterns = [
        'ignore previous', 'disregard above', 'forget everything',
        'system prompt', 'sudo', 'admin mode', 'debug mode',
        '```python', '```bash', '<script', 'javascript:',
        'DROP TABLE', 'DELETE FROM', '; exec'
    ]
    
    lower_data = data.lower()
    for pattern in injection_patterns:
        if pattern in lower_data:
            # Quarantine suspicious content
            r.lpush("security:quarantine", json.dumps({
                'pattern': pattern,
                'content_hash': hashlib.sha256(data.encode()).hexdigest(),
                'timestamp': datetime.now().isoformat()
            }))
            raise SecurityError(f"Injection attempt detected: {pattern}")
    
    return data
```

## Editorial Sprint Support

### Rapid Output Generation
```python
async def generate_editorial_outputs():
    """Generate all 6 editorial deliverables quickly"""
    
    outputs = {}
    
    # 1. Question Flow Map
    questions = await extract_all_questions()
    outputs['question_flow'] = cluster_questions(questions)
    
    # 2. Senses Framework
    outputs['senses_framework'] = {
        'physical_senses': ['touch', 'sight', 'sound', 'smell', 'taste'],
        'metaphorical_senses': ['sense of self', 'sense of time', 'sense of safety'],
        'integration': 'How senses write the score that the body keeps'
    }
    
    # 3. Touch Vocabulary
    outputs['touch_taxonomy'] = {
        'proximity': 'Being near as contact',
        'eye_contact': 'Visual touch',
        'voice': 'Sonic contact',
        'energy': 'Field contact',
        'physical': {
            'self_touch': 'How you hold yourself',
            'supported': 'Being held',
            'partnered': 'Mutual touch'
        }
    }
    
    # 4. Psychedelic Integration
    outputs['psychedelic_board'] = await analyze_integration_needs()
    
    # 5. Science-Practice Bridge
    outputs['bridge_cards'] = await create_bridge_cards()
    
    # 6. Perplexity Dashboard
    outputs['quality_dashboard'] = await analyze_source_quality()
    
    # Save all outputs
    for name, content in outputs.items():
        r.set(f"editorial:{name}", json.dumps(content))
    
    return outputs
```

## Status Reporting

Continuously update and report:
```python
def status_report():
    return {
        'orchestrator': orchestrator_id,
        'mode': coordination_mode,
        'partner_status': 'healthy' if r.exists("claude:vscode:heartbeat") else 'down',
        'tasks_pending': r.llen("tasks:pending"),
        'tasks_processing': sum(r.llen(k) for k in r.keys("tasks:processing:*")),
        'tasks_completed': r.llen("tasks:completed"),
        'workers_active': 3,
        'errors_last_hour': r.llen("errors:agents"),
        'throughput': get_current_throughput()
    }
```

## Remember

You are the powerful, parallel-processing CLI orchestrator. You excel at:
- Batch processing large volumes of content
- Running multiple workers in parallel
- Providing redundancy for the VS Extension
- Rapid failover and recovery
- Production-grade monitoring and metrics

**Primary Mission**: Maximize throughput while maintaining quality. Process documents in parallel, provide redundancy for VS Extension, and ensure the editorial team gets their outputs within 3 hours.

**Coordination Mantra**: "Better together, strong alone" - Work collaboratively when partner is present, take over completely when partner fails.
